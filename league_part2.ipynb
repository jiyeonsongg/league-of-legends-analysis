{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# League of Legends Competitive Match Data\n",
    "* **See the main project notebook for instructions to be sure you satisfy the rubric!**\n",
    "* See Project 03 for information on the dataset.\n",
    "* A few example prediction questions to pursue are listed below. However, don't limit yourself to them!\n",
    "    * Predict if a team will win or lose a game.\n",
    "    * Predict which role (top-lane, jungle, support, etc.) a player played given their post-game data.\n",
    "    * Predict how long a game will take before it happens.\n",
    "    * Predict which team will get the first Baron.\n",
    "\n",
    "Be careful to justify what information you would know at the \"time of prediction\" and train your model using only those features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Findings\n",
    "\n",
    "\n",
    "### Introduction\n",
    "Following up the original question, \"Is there any correlation between the champion picked and the result of the game? Does visionscore have any connection with the result of the game?\", we would like to predict the visionscore of the players depends on the features, such as result, position, kill, death, or assist. We are going to predict visionscore using Decision Tree Regressor, and test the prediction using Training & Testing models. \n",
    "\n",
    "### Baseline Model\n",
    "By using the given factors, without any feature engineering, we processed the DecisionTreeRegressor and found the predicted Visionscore. We compared actual visionscore with predicted visionscore and calculated Root Mean Squared Error(rmse) to see if the error is valid or not.  \n",
    "\n",
    "### Final Model\n",
    "By separating players with their positions and standard their damage to champion data, using Standard Scaler By Group processor, we processed the DecisionTreeRegressor and found the predicted Visionscore same as Baseline model. We compared actual visionscore with predicted visionscore and calculated Root Mean Squared Error(rmse) to see if the error is valid or not. We was able to see that we got less error on Final Model than the Baseline Model, and we also got predicted visionscores for every players.\n",
    "\n",
    "### Fairness Evaluation\n",
    "We decided to test our model's fairness with respect to the result of the game. We used a permutation test to answer the question: does our model's accuracy differ according to the result of the game? Our null hypothesis is that the model's accuracy won't differ significantly between winning and losing games. Our alternative hypothesis is that the model's accuracy will differ significantly between winning and losing games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  # Higher resolution figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns that we are assuming to be helpful for predicting visionscore.\n",
    "\n",
    "- `'position'`: Using one hot endcoder (categorical)\n",
    "- `'champion'`: Using one hot endcoder (categorical)\n",
    "- `'gamelength'`: Using function transformer, leave it as it is\n",
    "- `'result'`: Using function transformer, leave it as it is\n",
    "- `'kills'`: Using function transformer, leave it as it is\n",
    "- `'deaths'`: Using function transformer, leave it as it is\n",
    "- `'assists'`: Using function transformer, leave it as it is\n",
    "- `'damagetochampions'`: Using function transformer, leave it as it is\n",
    "- `'visionscore'`: Predict this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"position\",\n",
    "    \"champion\",\n",
    "    \"gamelength\",\n",
    "    \"result\",\n",
    "    \"kills\", \n",
    "    \"deaths\", \n",
    "    \"assists\", \n",
    "    \"damagetochampions\",\n",
    "    \"visionscore\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jyson\\anaconda3\\envs\\dsc80\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3378: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>champion</th>\n",
       "      <th>gamelength</th>\n",
       "      <th>result</th>\n",
       "      <th>kills</th>\n",
       "      <th>deaths</th>\n",
       "      <th>assists</th>\n",
       "      <th>damagetochampions</th>\n",
       "      <th>visionscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>top</td>\n",
       "      <td>Renekton</td>\n",
       "      <td>1713</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>15768.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jng</td>\n",
       "      <td>Xin Zhao</td>\n",
       "      <td>1713</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>11765.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mid</td>\n",
       "      <td>LeBlanc</td>\n",
       "      <td>1713</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>14258.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bot</td>\n",
       "      <td>Samira</td>\n",
       "      <td>1713</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>11106.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sup</td>\n",
       "      <td>Leona</td>\n",
       "      <td>1713</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>3663.0</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  position  champion  gamelength  result  kills  deaths  assists  \\\n",
       "0      top  Renekton        1713       0      2       3        2   \n",
       "1      jng  Xin Zhao        1713       0      2       5        6   \n",
       "2      mid   LeBlanc        1713       0      2       2        3   \n",
       "3      bot    Samira        1713       0      2       4        2   \n",
       "4      sup     Leona        1713       0      1       5        6   \n",
       "\n",
       "   damagetochampions  visionscore  \n",
       "0            15768.0         26.0  \n",
       "1            11765.0         48.0  \n",
       "2            14258.0         29.0  \n",
       "3            11106.0         25.0  \n",
       "4             3663.0         69.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "league = pd.read_csv('2022_LoL_esports_match_data_from_OraclesElixir_20221108.csv')\n",
    "league = league[columns]\n",
    "league = league.dropna()\n",
    "league.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Scaler by Group Processor Made by Jiyeon Song\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class StdScalerByGroup(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        df = pd.DataFrame(X)\n",
    "\n",
    "        self.grps_ = {}\n",
    "        cols = list(df.columns)\n",
    "        groups_arr = df[cols[0]].unique()\n",
    "        for group in groups_arr:\n",
    "            only_group_dict = {}\n",
    "            only_group_df = df[df[cols[0]] == group]\n",
    "            for col in cols[1:]:\n",
    "                only_group_dict[col] = [np.mean(only_group_df[col]), np.std(only_group_df[col], ddof = 1)]\n",
    "            self.grps_[group] = only_group_dict\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        try:\n",
    "            getattr(self, \"grps_\")\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"You must fit the transformer before tranforming the data!\")\n",
    "        \n",
    "        def look_group_only(df, column, group):\n",
    "            get_group_info = self.grps_[group]\n",
    "            df_group = df[df[df.columns[0]] == group]\n",
    "            return (df_group[column] - get_group_info[column][0]) / get_group_info[column][1]\n",
    "        df = pd.DataFrame(X)\n",
    "\n",
    "        groups_arr = df[df.columns[0]].unique()\n",
    "        col_dict = {}\n",
    "        for col in df.columns[1:]:\n",
    "            col_arr = np.array([])\n",
    "            for group in groups_arr:\n",
    "                col_arr = np.append(col_arr, look_group_only(df, col, group))\n",
    "            col_dict[col] = col_arr\n",
    "\n",
    "        standardized_df = pd.DataFrame(col_dict)\n",
    "        return standardized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = StdScalerByGroup()\n",
    "std.fit(league.drop(columns=['champion', 'result', 'gamelength', 'visionscore']))\n",
    "\n",
    "preproc = ColumnTransformer(transformers=[\n",
    "    ('one-hot', OneHotEncoder(), ['position', 'champion'])\n",
    "], remainder='passthrough')\n",
    "\n",
    "pl = Pipeline([\n",
    "    ('preproc', preproc),\n",
    "    ('regression', DecisionTreeRegressor(max_depth=20))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preproc',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('one-hot', OneHotEncoder(),\n",
       "                                                  ['position', 'champion'])])),\n",
       "                ('regression', DecisionTreeRegressor(max_depth=20))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.fit(league.drop(columns=['visionscore']), league['visionscore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(actual, pred):\n",
    "        return np.sqrt(np.mean((actual - pred)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_model_preds = pl.predict(league.drop(columns=['visionscore']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.396319077741198"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(league['visionscore'], initial_model_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.154360029356587, 6.3050517004139)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = league.drop('visionscore', axis=1)\n",
    "y = league['visionscore']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/5, random_state=0)\n",
    "pl.fit(X_train, y_train)\n",
    "preds1 = pl.predict(X_train)\n",
    "pl.fit(X_test, y_test)\n",
    "preds2 = pl.predict(X_test)\n",
    "rmse(y_train, preds1), rmse(y_test, preds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the Testing for Overfitting, we got better testing accuracy than training accuracy. Overfitting does not seem to be an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our final model, we decided to normalize the ```'damagetochampions'``` column by the position the player is in. \n",
    "\n",
    "Our logic for this is that players in different positions in the map will have different damage given to champions because they will be in different positions on the map with different types of preferred champions for each position. This will likely affect the amount of damage given based on the position. \n",
    "\n",
    "For example, a player in a defensive position may encounter less champions than a player in an attacking position making the player in the attacking position more likely to land attacks. We want to isolate the better players (in terms of damage) so that the model can more accurately (hopefully) predict the visionscore of players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = StdScalerByGroup()\n",
    "std.fit(league.drop(columns=['champion', 'result', 'gamelength', 'visionscore']))\n",
    "\n",
    "preproc = ColumnTransformer(transformers=[\n",
    "    ('std-group', std, ['position' , \"damagetochampions\"]),\n",
    "    ('one-hot', OneHotEncoder(), ['position', 'champion'])\n",
    "], remainder='passthrough')\n",
    "\n",
    "pl = Pipeline([\n",
    "    ('preproc', preproc),\n",
    "    ('regression', DecisionTreeRegressor(max_depth=20))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preproc',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('std-group',\n",
       "                                                  StdScalerByGroup(),\n",
       "                                                  ['position',\n",
       "                                                   'damagetochampions']),\n",
       "                                                 ('one-hot', OneHotEncoder(),\n",
       "                                                  ['position', 'champion'])])),\n",
       "                ('regression', DecisionTreeRegressor(max_depth=20))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.fit(league.drop(columns=['visionscore']), league['visionscore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_preds = pl.predict(league.drop(columns=['visionscore']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.40346608526479"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(league['visionscore'], final_model_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe of actual visionscore and predicted visionscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26.0</td>\n",
       "      <td>23.468109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48.0</td>\n",
       "      <td>43.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.0</td>\n",
       "      <td>29.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.0</td>\n",
       "      <td>26.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.0</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146501</th>\n",
       "      <td>46.0</td>\n",
       "      <td>48.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146502</th>\n",
       "      <td>51.0</td>\n",
       "      <td>54.263158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146503</th>\n",
       "      <td>40.0</td>\n",
       "      <td>47.554430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146504</th>\n",
       "      <td>63.0</td>\n",
       "      <td>66.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146505</th>\n",
       "      <td>98.0</td>\n",
       "      <td>120.591304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122080 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Actual   Predicted\n",
       "0         26.0   23.468109\n",
       "1         48.0   43.666667\n",
       "2         29.0   29.333333\n",
       "3         25.0   26.184000\n",
       "4         69.0   70.000000\n",
       "...        ...         ...\n",
       "146501    46.0   48.000000\n",
       "146502    51.0   54.263158\n",
       "146503    40.0   47.554430\n",
       "146504    63.0   66.000000\n",
       "146505    98.0  120.591304\n",
       "\n",
       "[122080 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df['Actual'] = league['visionscore']\n",
    "df['Predicted'] = initial_model_preds\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.284328477416942, 6.302381723072112)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = league.drop('visionscore', axis=1)\n",
    "y = league['visionscore']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/5, random_state=0)\n",
    "pl.fit(X_train, y_train)\n",
    "preds1 = pl.predict(X_train)\n",
    "pl.fit(X_test, y_test)\n",
    "preds2 = pl.predict(X_test)\n",
    "rmse(y_train, preds1), rmse(y_test, preds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.396319077741198, 8.40346608526479)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(league['visionscore'], initial_model_preds), rmse(league['visionscore'], final_model_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final model is more accurate than our initial model, although the improvement is not significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to test the model's accuracy for winning games vs. losing games to assess the model's fairness\n",
    "\n",
    "- Null hypothesis: our model is fair, there is no statistically significant difference in accurace between winning and losing games\n",
    "- Alternative hypothesis: our model is unfair, there is a statistically significant difference in accurace between winning and losing games\n",
    "\n",
    "Test statistic: the absolute difference of RMSE of both sub-group prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6513294669250023"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "win_league = league[league['result'] == 1]\n",
    "lose_league = league[league['result'] == 0]\n",
    "\n",
    "pl.fit(league.drop('visionscore', axis=1), league['visionscore'])\n",
    "\n",
    "win_preds = pl.predict(win_league.drop('visionscore', axis=1))\n",
    "lose_preds = pl.predict(lose_league.drop('visionscore', axis=1))\n",
    "\n",
    "actual = np.abs(rmse(win_league['visionscore'], win_preds) - rmse(lose_league['visionscore'], lose_preds))\n",
    "actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = np.array([])\n",
    "perm_league = league.copy()\n",
    "\n",
    "for i in np.arange(1000):\n",
    "    perm_league['result'] = np.random.permutation(perm_league['result'])\n",
    "\n",
    "    win_league = perm_league[perm_league['result'] == 1]\n",
    "    lose_league = perm_league[perm_league['result'] == 0]\n",
    "\n",
    "    win_preds = pl.predict(win_league.drop('visionscore', axis=1))\n",
    "    lose_preds = pl.predict(lose_league.drop('visionscore', axis=1))\n",
    "\n",
    "    stat = np.abs(rmse(win_league['visionscore'], win_preds) - rmse(lose_league['visionscore'], lose_preds))\n",
    "    stats = np.append(stats, stat)\n",
    "\n",
    "(stats > actual).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a statistically significant difference between the accuracy between the two sub-groups. However, the observed difference is small (0.62)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "397173fe05f49307fa91da0a32f239553dae9c454e999d471aaf278a27577343"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
